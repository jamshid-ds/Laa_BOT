# -*- coding: utf-8 -*-
"""laa_bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cfr0P2El1QlnZgCDU7VT-SMqF_MUl45h
"""

# %%capture
!pip install chromadb
!pip install openai==0.28
!pip install environs
!pip install langchain
!pip install tiktoken
!pip install telebot
!pip install pydub
!pip install SpeechRecognition
!pip install langchain-community
!pip install pypdf
!pip install gtts

import openai
from environs import Env
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.vectorstores import Chroma
from langchain.document_loaders import CSVLoader, word_document, ImageCaptionLoader
from langchain.document_loaders import PyPDFLoader

import telebot
import openai

from pydub import AudioSegment
import speech_recognition

def ogg_to_wav(audio):
  ogg_audio = AudioSegment.from_ogg(audio)
  return ogg_audio.export("voice.wav", format="wav")

def stt_func(audio):
  audio = ogg_to_wav(audio)
  recognizer = speech_recognition.Recognizer()
  with speech_recognition.AudioFile(audio) as source:
  # Convert the audio to a recognizable format
    audio = recognizer.record(source)
    text = recognizer.recognize_google(audio,language="ru-RU")
    return text

loader = PyPDFLoader("Latokent_template (1).pdf")
pages = loader.load()

embedding = OpenAIEmbeddings(openai_api_key="openai-key")
llm_name = "gpt-3.5-turbo"
llm = ChatOpenAI(model_name=llm_name, temperature=0, openai_api_key="openai-key")
print("Beginning")
vectordb = Chroma.from_documents(pages, embedding)
print('Database Loaded')

template_uz = """Use the context pieces below to answer the question at the end.

Please write all information in Russian. If the user asks about the latoken enterprise, answer it in a longer form.
answer even if the question is not about latoken. Remember, you are bot of LATOKEN

{context}
Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT_uz = PromptTemplate(input_variables=["context", "question"], template=template_uz)

# Run chain
qa_chain_uz = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever(), return_source_documents=False,
                                       chain_type_kwargs={"prompt": QA_CHAIN_PROMPT_uz})

def chatbot(question, history=False):
   return str(qa_chain_uz({"query": question})['result'])

import gtts

def text_to_speech(msg):
    tts = gtts.gTTS(msg, lang='ru')
    tts.save('LATOKEN_AI.mp3')

import telebot
import openai
import speech_recognition


openai.api_key = "openai-key'


def GPT_chat(prompt):

    completion = openai.ChatCompletion.create(
        model = 'gpt-3.5-turbo',
        messages =  [{"role":'user', 'content':prompt}]
    )


    return  completion.choices[0].message.content




TOKEN = "bot-token"
bot = telebot.TeleBot(TOKEN, parse_mode=None)

@bot.message_handler(commands=['start'])
def send_welcome(message):
    bot.reply_to(message, "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –¥–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –±–æ—Ç-–ø–æ–º–æ—â–Ω–∏–∫ Latoken.")

@bot.message_handler(commands=['test'])
def send_welcome(message):
    bot.reply_to(message, "TEST FUNCTION")


@bot.message_handler(content_types=['voice'])
def duck(message):
    file = bot.get_file(message.voice.file_id)
    bytes = bot.download_file(file.file_path)
    with open('voice.ogg', 'wb') as f:
        f.write(bytes)
    text = stt_func("voice.ogg")
    text = chatbot(text)
    text_to_speech(text)
    bot.send_message(message.chat.id, text=text)
    with open('LATOKEN_AI.mp3', 'rb') as f:
        bot.send_audio(message.chat.id, f)



@bot.message_handler(func=lambda message: True)
def echo_all(message):
  sent_message = bot.reply_to(message, text='üöÄ')
  bot.reply_to(message, GPT_chat(message.text))
  bot.delete_message(chat_id=message.chat.id, message_id=sent_message.message_id)

bot.polling()

